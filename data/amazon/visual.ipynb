{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab1792ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open(\"/net/winter/work/shimizu/Lab/LLM4CDSR-pytorch/data/amazon/handled/user_profile.pkl\", \"rb\") as f:\n",
    "    user_profile = pickle.load(f)\n",
    "\n",
    "print(user_profile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7722c276",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_profile[11]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f4695ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/net/winter/work/shimizu/Lab/LLM4CDSR-pytorch/data/amazon/handled/cloth_sport_fashion.pkl\", \"rb\") as f:\n",
    "    cloth_sport_fashion = pickle.load(f)\n",
    "\n",
    "len(cloth_sport_fashion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed17d796",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(cloth_sport_fashion[0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "352c0db2",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/net/winter/work/shimizu/Lab/LLM4CDSR-pytorch/data/amazon/handled/amazon_all.pkl\", \"rb\") as f:\n",
    "    amazon_all = pickle.load(f)\n",
    "\n",
    "len(amazon_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1f9f82d",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(amazon_all[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e20f2a99",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "import json\n",
    "from data_process import New_Amazon, Amazon_meta, stream_amazon_reviews_to_parquet\n",
    "from collections import defaultdict\n",
    "import pandas as pd\n",
    "import config\n",
    "\n",
    "class AmazonHandler():\n",
    "    def __init__(self):\n",
    "        self.domains_map = config.DOMAINS\n",
    "        self.user_core = config.USER_CORE\n",
    "        self.item_core = config.ITEM_CORE\n",
    "\n",
    "        self.df = pd.DataFrame()\n",
    "        # クラス内で使用する変数をすべて初期化\n",
    "        self.final_data = {}\n",
    "        self.final_domain = {}\n",
    "        self.user_dict = {}\n",
    "        self.item_dict = {}\n",
    "\n",
    "        os.makedirs(config.HANDLE_DATA_DIR, exist_ok=True)\n",
    "\n",
    "    def run_pipeline(self):\n",
    "        print(\"--- 1.1 : Loading and Combining Data ---\")\n",
    "        #self.load_data()\n",
    "        self._load_and_combine_data()\n",
    "\n",
    "        print(\"--- 1.2 : Filtering Data by Time ---\")\n",
    "        #self.filter_time(t_min=config.TIME_MIN, t_max = config.TIME_MAX)\n",
    "\n",
    "        self._filter_time(t_min=config.TIME_MIN, t_max = config.TIME_MAX)\n",
    "\n",
    "        #self.filter_Kcore(user_core=self.user_core, item_core=self.item_core)\n",
    "        self._filter_Kcore()\n",
    "\n",
    "        print(\"--- 1.3 : Counting Interactions ---\")\n",
    "        #self.id_map()\n",
    "        self._id_map()\n",
    "\n",
    "        print(\"--- Step 5 : Saving Data ---\")\n",
    "\n",
    "        # with open(f\"{config.HANDLE_DATA_DIR}/id_map.json\", \"w\") as f:\n",
    "        #     all_maps = {\"user_dict\":self.user_dict, \"item_dict\":self.item_dict}\n",
    "        #     json.dump(all_maps, f, indent=4)\n",
    "        #     print(f\"Saved id_map.json\")\n",
    "        \n",
    "        # with open(f\"{config.HANDLE_DATA_DIR}/amazon_all.pkl\", \"wb\") as f:\n",
    "        #     pickle.dump((self.final_data, self.final_domain), f)\n",
    "        #     print(f\"Saved amazon_all.pkl\")\n",
    "\n",
    "    #    self._save_results()\n",
    "\n",
    "    #     self.export_sequences_for_domains(\n",
    "    #         domain_ids=[0, 1, 2], \n",
    "    #         output_path=f\"{config.HANDLE_DATA_DIR}/cloth_sport_fashion.pkl\"\n",
    "    #     )\n",
    "\n",
    "        # self.fetch_metadata(\n",
    "        #     domain_ids=[0, 1, 2],\n",
    "        #     output_path=f\"{config.HANDLE_DATA_DIR}/\"\n",
    "        # )\n",
    "\n",
    "        print(\"\\n--- Processing Summary ---\")\n",
    "\n",
    "    def _load_and_combine_data(self):\n",
    "        all_dfs = []\n",
    "        for domein_id_str, domain_name in self.domains_map.items():\n",
    "            parquet_path = f'./raw/{domain_name}.parquet'\n",
    "            if not os.path.exists(parquet_path):\n",
    "                print(f\"Parquet file not found for domain {domain_name}\")\n",
    "                stream_amazon_reviews_to_parquet(domain_name,rating_score=0)\n",
    "            else:\n",
    "                print(\"Found parquet file for domain {domain_name}\")\n",
    "            \n",
    "            df_domain = pd.read_parquet(parquet_path)\n",
    "            df_domain['domain_id'] = int(domein_id_str)\n",
    "            all_dfs.append(df_domain)\n",
    "        \n",
    "        self.df = pd.concat(all_dfs, ignore_index=True)\n",
    "        self.df[\"time\"] = pd.to_numeric(self.df[\"time\"]).astype(np.int64)\n",
    "        self.df[\"domain_id\"] = self.df[\"domain_id\"].astype(np.int8)\n",
    "        print(f\"Data loaded. Total:{len(self.df)} interactions\")\n",
    "\n",
    "\n",
    "    def _filter_time(self, t_min, t_max):\n",
    "        initial_count = len(self.df)\n",
    "        self.df = self.df[(self.df[\"time\"] > t_min) & (self.df[\"time\"] < t_max)]\n",
    "        print(f\"Time filter done!. first:{initial_count} ,Interactions remaining: { len(self.df) }\")    \n",
    "    \n",
    "    def _filter_Kcore(self):\n",
    "        while True:\n",
    "            initial_count = len(self.df)\n",
    "            user_counts = self.df.groupby('user')['user'].transform('size')\n",
    "            item_counts = self.df.groupby('item')['item'].transform('size')\n",
    "\n",
    "            mask = (user_counts >= self.user_core) & (item_counts >= self.item_core)\n",
    "            self.df = self.df[mask]\n",
    "\n",
    "            print(f\"Filtering iteration ... Interactions remaining: { len(self.df) }\")\n",
    "            if len(self.df) == initial_count:\n",
    "                print(\" K-core condition met.\")\n",
    "                break\n",
    "        print(f\"K-core filter complete.\\n\")\n",
    "    \n",
    "    def _id_map(self):\n",
    "        if self.df.empty:\n",
    "            print(\"No data to map.\")\n",
    "            return\n",
    "        unique_users = self.df['user'].unique()\n",
    "        self.user_dict = {\n",
    "        \"str2id\": {user_str: idx+1 for idx, user_str in enumerate(unique_users)},\n",
    "        \"id2str\": {idx + 1 : user_str for idx, user_str in enumerate(unique_users)}\n",
    "        }\n",
    "\n",
    "        self.item_dict = {int(key): {\"str2id\": {}, \"id2str\": {}} for key in self.domains_map.keys()}\n",
    "        for domain_id, group in self.df.groupby('domain_id'):\n",
    "            unique_items = group['item'].unique()\n",
    "            self.item_dict[domain_id] = {\n",
    "                'str2id': {item_str: i + 1 for i, item_str in enumerate(unique_items)},\n",
    "                'id2str': {i + 1: item_str for i, item_str in enumerate(unique_items)}\n",
    "            }\n",
    " \n",
    "        self.df['new_user_id'] = self.df['user'].map(self.user_dict['str2id'])\n",
    "        self.df['new_item_id'] = self.df.apply(\n",
    "            lambda row: self.item_dict[row['domain_id']]['str2id'][row['item']], axis=1)\n",
    "\n",
    "        \n",
    "        sorted_df = self.df.sort_values(by=['new_user_id', 'time'])\n",
    "        grouped = sorted_df.groupby('new_user_id')\n",
    "        for user_id, group in tqdm(grouped, desc=\"    Building final sequences\"):\n",
    "            self.final_data[user_id] = group['new_item_id'].tolist()\n",
    "            self.final_domain[user_id] = group['domain_id'].tolist()\n",
    "\n",
    "        self.item_dict[\"item_count\"] = {key: len(val[\"str2id\"]) for key, val in self.item_dict.items() if isinstance(val, dict)}\n",
    "        print(\"  Sequence building complete.\")\n",
    "\n",
    "    def _save_results(self):\n",
    "        id_map_path = os.path.join(config.HANDLE_DATA_DIR, \"id_map.json\")\n",
    "        item_dict_str_keys = {str(k): v for k, v in self.item_dict.items()}\n",
    "        with open(id_map_path, \"w\") as f:\n",
    "            json.dump({\"user_dict\":self.user_dict, \"item_dict\":item_dict_str_keys}, f, indent=4)\n",
    "        \n",
    "        inter_path = os.path.join(config.HANDLE_DATA_DIR, \"amazon_all.pkl\")\n",
    "        with open(inter_path, \"wb\") as f:\n",
    "            pickle.dump((self.final_data, self.final_domain), f)\n",
    "    \n",
    "\n",
    "    def export_sequences_for_domains(self,domain_ids: list, output_path: str):\n",
    "        print(f\"--- Exporting sequences for domains {domain_ids} --- \")\n",
    "        if not self.final_data: \n",
    "            print(\"No sequences available. Please run id_map() first.\")\n",
    "            return\n",
    "        \n",
    "        filtered_data = {}\n",
    "        filtered_domain = {}\n",
    "        for user_id, domain_seq in tqdm(self.final_domain.items(), desc=\"Filetering sequences\"):\n",
    "            domain_seq_np = np.array(domain_seq)\n",
    "            item_seq_np = np.array(self.final_data[user_id])\n",
    "\n",
    "            mask = np.isin(domain_seq_np, domain_ids)\n",
    "            if np.any(mask):\n",
    "                filtered_data[user_id] = item_seq_np[mask].tolist()\n",
    "                filtered_domain[user_id] = domain_seq_np[mask].tolist()\n",
    "            \n",
    "        print(f\"Export complete. {len(filtered_data)} users have sequences with the specified domains.\")\n",
    "        with open(output_path, \"wb\") as f:\n",
    "            pickle.dump((filtered_data, filtered_domain), f)\n",
    "        print(f\"Saved {output_path}\")\n",
    "\n",
    "    def fetch_metadata(self,domain_ids: list, output_path: str):\n",
    "        print(f\"--- Fetching metadata for domains {domain_ids} ---\")\n",
    "        for domain_id in domain_ids:\n",
    "            meta_data = Amazon_meta(self.domains_map[str(domain_id)], self.item_dict[domain_id])\n",
    "            json_str = json.dumps(meta_data)\n",
    "\n",
    "            with open(f\"`{output_path}item2attributes_{domain_id}.json\", 'w') as out:\n",
    "                out.write(json_str)\n",
    "        return\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "08cf5489",
   "metadata": {},
   "outputs": [],
   "source": [
    "parquet_path = f'/net/winter/work/shimizu/Lab/LLM4CDSR-pytorch/data/amazon/raw/Clothing_Shoes_and_Jewelry.parquet'\n",
    "df_domain = pd.read_parquet(parquet_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "efb91f2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _filter_time(df, t_min, t_max):\n",
    "   initial_count = len(df)\n",
    "   df = df[(df[\"time\"] > t_min) & (df[\"time\"] < t_max)]\n",
    "   print(f\"Time filter done!. Interactions remaining: { len(df) }\")    \n",
    "   return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7f2ab8c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _filter_Kcore(df,user_core,item_core):\n",
    "    while True:\n",
    "        initial_count = len(df)\n",
    "        user_counts = df.groupby('user')['user'].transform('size')\n",
    "        item_counts = df.groupby('item')['item'].transform('size')\n",
    "\n",
    "        mask = (user_counts > user_core) & (item_counts > item_core)\n",
    "        df = df[mask]\n",
    "\n",
    "        print(f\"Filtering iteration ... Interactions remaining: { len(df) }\")\n",
    "        if len(df) == initial_count:\n",
    "            print(\" K-core condition met.\")\n",
    "            break\n",
    "    print(f\"K-core filter complete.\\n\")\n",
    "    return df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cd092e53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time filter done!. Interactions remaining: 2932561\n"
     ]
    }
   ],
   "source": [
    "\n",
    "TIME_MIN = 1514736000  # 2018-01-01 00:00:00 GMT\n",
    "TIME_MAX = 1577808000 \n",
    "df = _filter_time(df_domain,TIME_MIN,TIME_MAX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bb1924e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtering iteration ... Interactions remaining: 293814\n",
      "Filtering iteration ... Interactions remaining: 191905\n",
      "Filtering iteration ... Interactions remaining: 138783\n",
      "Filtering iteration ... Interactions remaining: 121700\n",
      "Filtering iteration ... Interactions remaining: 107643\n",
      "Filtering iteration ... Interactions remaining: 101560\n",
      "Filtering iteration ... Interactions remaining: 95922\n",
      "Filtering iteration ... Interactions remaining: 93120\n",
      "Filtering iteration ... Interactions remaining: 90193\n",
      "Filtering iteration ... Interactions remaining: 88521\n",
      "Filtering iteration ... Interactions remaining: 86734\n",
      "Filtering iteration ... Interactions remaining: 85706\n",
      "Filtering iteration ... Interactions remaining: 84554\n",
      "Filtering iteration ... Interactions remaining: 83933\n",
      "Filtering iteration ... Interactions remaining: 83213\n",
      "Filtering iteration ... Interactions remaining: 82794\n",
      "Filtering iteration ... Interactions remaining: 82281\n",
      "Filtering iteration ... Interactions remaining: 81950\n",
      "Filtering iteration ... Interactions remaining: 81567\n",
      "Filtering iteration ... Interactions remaining: 81361\n",
      "Filtering iteration ... Interactions remaining: 81093\n",
      "Filtering iteration ... Interactions remaining: 80901\n",
      "Filtering iteration ... Interactions remaining: 80681\n",
      "Filtering iteration ... Interactions remaining: 80527\n",
      "Filtering iteration ... Interactions remaining: 80315\n",
      "Filtering iteration ... Interactions remaining: 80174\n",
      "Filtering iteration ... Interactions remaining: 80074\n",
      "Filtering iteration ... Interactions remaining: 79978\n",
      "Filtering iteration ... Interactions remaining: 79917\n",
      "Filtering iteration ... Interactions remaining: 79832\n",
      "Filtering iteration ... Interactions remaining: 79754\n",
      "Filtering iteration ... Interactions remaining: 79649\n",
      "Filtering iteration ... Interactions remaining: 79580\n",
      "Filtering iteration ... Interactions remaining: 79499\n",
      "Filtering iteration ... Interactions remaining: 79447\n",
      "Filtering iteration ... Interactions remaining: 79339\n",
      "Filtering iteration ... Interactions remaining: 79286\n",
      "Filtering iteration ... Interactions remaining: 79180\n",
      "Filtering iteration ... Interactions remaining: 79116\n",
      "Filtering iteration ... Interactions remaining: 79039\n",
      "Filtering iteration ... Interactions remaining: 78989\n",
      "Filtering iteration ... Interactions remaining: 78919\n",
      "Filtering iteration ... Interactions remaining: 78856\n",
      "Filtering iteration ... Interactions remaining: 78788\n",
      "Filtering iteration ... Interactions remaining: 78731\n",
      "Filtering iteration ... Interactions remaining: 78684\n",
      "Filtering iteration ... Interactions remaining: 78639\n",
      "Filtering iteration ... Interactions remaining: 78613\n",
      "Filtering iteration ... Interactions remaining: 78573\n",
      "Filtering iteration ... Interactions remaining: 78567\n",
      "Filtering iteration ... Interactions remaining: 78562\n",
      "Filtering iteration ... Interactions remaining: 78562\n",
      " K-core condition met.\n",
      "K-core filter complete.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = _filter_Kcore(df,5,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f3a2c8e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "B0001YRE04    1504\n",
       "B00028AVDG    1500\n",
       "B000YXC2LI    1475\n",
       "B015YA2Z26    1361\n",
       "B00UFJS0I0    1174\n",
       "              ... \n",
       "B000JXOSYC       4\n",
       "B000JXN21W       4\n",
       "B00NB91Y7U       4\n",
       "B00NACUBY0       4\n",
       "B01HF4AQ48       4\n",
       "Name: item, Length: 1343, dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['item'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d4b7a37",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_Kcore(data, user_core, item_core):\n",
    "    print(\"  Applying K-core filter (original list-based function)...\")\n",
    "    new_data = list(data)\n",
    "    while True:\n",
    "        user_count, item_count = defaultdict(int), defaultdict(int)\n",
    "        for inter in new_data:\n",
    "            user_count[inter[0]] += 1\n",
    "            item_count[inter[1]] += 1\n",
    "        \n",
    "        # フィルタリング条件 (元のコードのロジックを忠実に再現)\n",
    "        # ここでは >= ではなく > を使っている点に注意\n",
    "        core_users = {u for u, c in user_count.items() if c > user_core}\n",
    "        core_items = {i for i, c in item_count.items() if i > item_core}\n",
    "        \n",
    "        initial_count = len(new_data)\n",
    "        new_data = [\n",
    "            inter for inter in new_data \n",
    "            if inter[0] in core_users and inter[1] in core_items\n",
    "        ]\n",
    "        print(f\"    Filtering iteration... Interactions remaining: {len(new_data)}\")\n",
    "        if len(new_data) == initial_count:\n",
    "            break\n",
    "            \n",
    "    return new_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "483cbf77",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/net/winter/work/shimizu/Lab/LLM4CDSR-pytorch/data/amazon/handled/partioned_user_sequences.pkl'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpickle\u001b[39;00m \n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/net/winter/work/shimizu/Lab/LLM4CDSR-pytorch/data/amazon/handled/partioned_user_sequences.pkl\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m      3\u001b[0m     partioned_user_sequences \u001b[38;5;241m=\u001b[39m pickle\u001b[38;5;241m.\u001b[39mload(f)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28mlen\u001b[39m(partioned_user_sequences)\n",
      "File \u001b[0;32m~/.conda/envs/LLM4CDSR/lib/python3.9/site-packages/IPython/core/interactiveshell.py:284\u001b[0m, in \u001b[0;36m_modified_open\u001b[0;34m(file, *args, **kwargs)\u001b[0m\n\u001b[1;32m    277\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[1;32m    278\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    279\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    280\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    281\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    282\u001b[0m     )\n\u001b[0;32m--> 284\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/net/winter/work/shimizu/Lab/LLM4CDSR-pytorch/data/amazon/handled/partioned_user_sequences.pkl'"
     ]
    }
   ],
   "source": [
    "import pickle \n",
    "with open(\"/net/winter/work/shimizu/Lab/LLM4CDSR-pytorch/data/amazon/handled/partitioned_user_sequences.pkl\", \"rb\") as f:\n",
    "    partioned_user_sequences = pickle.load(f)\n",
    "\n",
    "len(partioned_user_sequences)\n",
    "partioned_user_sequences[0]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb83f35c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LLM4CDSR",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
